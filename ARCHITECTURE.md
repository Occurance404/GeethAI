# Telugu AI Music System Architecture

This document outlines the proposed architecture for the Telugu AI Music and Lyrics Generation System. The design is modular, separating the lyrical and musical generation processes to allow for independent development, training, and improvement.

## High-Level Overview

The system is designed as a two-stage pipeline. First, a specialized language model generates lyrics. Second, a conditional audio model generates music and vocals based on those lyrics.

```
+-----------------+      +----------------------+      +---------------------------+      +------------------+      +----------+
|  Optional User  |----->|  Kavi (Lyrics LLM)   |----->|  Sangeethakartha (AudioLM)  |----->|  EnCodec Decoder |----->|  .WAV    |
|      Prompt     |      | (Fine-tuned Mistral) |      | (Custom Transformer)      |      | (Tokens to Audio)|      |   Song   |
+-----------------+      +----------------------+      +---------------------------+      +------------------+      +----------+
                             |                                  ^
                             | (Generated Lyrics)               | (Lyrics as a Condition)
                             +----------------------------------+
```

---

## Component Breakdown

### 1. Kavi: The Lyrics Generation Model

*   **Purpose:** To generate high-quality, coherent, and contextually relevant lyrics in Telugu.
*   **Core Technology:** A fine-tuned Large Language Model (LLM).
*   **Proposed Model:** **Mistral-7B** (or a similar high-performance, open-source model). It offers a great balance of capability and resource efficiency.
*   **Input:** An optional user prompt (e.g., "a romantic song about a moonlit night") or a command to generate lyrics on a random theme.
*   **Output:** A structured text or JSON object containing the song lyrics, potentially broken down into sections like `pallavi` (chorus) and `charanam` (verse).
*   **Training:** Kavi will be fine-tuned on a curated dataset comprising:
    1.  A large corpus of Telugu song lyrics.
    2.  Telugu poetry and classical literature to enrich its vocabulary and stylistic range.

### 2. EnCodec: The Audio Tokenizer

*   **Purpose:** To translate continuous audio waveforms into a sequence of discrete numerical tokens, and vice-versa. This allows us to treat audio generation as a "language modeling" problem.
*   **Core Technology:** A neural audio codec, specifically **EnCodec** from Meta.
*   **Process:**
    *   **Encoding (for Training):** `audio.wav -> EnCodec Encoder -> [23, 101, 5, ...]`
    *   **Decoding (for Inference):** `[42, 99, 18, ...] -> EnCodec Decoder -> generated_song.wav`

### 3. Sangeethakartha: The Audio & Music Generation Model

*   **Purpose:** To compose music and a vocal melody that aligns with the provided lyrics.
*   **Core Technology:** A custom **Decoder-only Transformer** model, architecturally similar to a GPT model but designed for audio tokens.
*   **Input:** The model is *conditioned* on the lyrics generated by Kavi. It receives:
    1.  The tokenized lyrics.
    2.  A special "start of audio" token.
*   **Architecture:**
    *   The model will use **cross-attention**. While it generates the next audio token in the sequence, the cross-attention mechanism will allow it to "look at" the lyric tokens. This is how the model learns to align the melody and singing with the meaning and rhythm of the words.
*   **Output:** A single, unified stream of audio tokens representing both the instrumental music and the sung vocals.
*   **Training:** Sangeethakartha will be trained on a parallel dataset of `(lyrics, audio_tokens)` pairs. It learns to predict the next audio token based on the preceding audio tokens and the full lyrical context.

---

## Inference Pipeline (Putting It All Together)

1.  A user provides an optional prompt (e.g., "create a folk song about harvest").
2.  **Kavi** receives the prompt and generates a full set of Telugu lyrics.
3.  The lyrics are tokenized and passed as a condition to **Sangeethakartha**.
4.  Sangeethakartha autoregressively generates a complete sequence of audio tokens, from the beginning of the song to the end.
5.  The generated audio tokens are passed to the **EnCodec Decoder**.
6.  The decoder converts the tokens into a final `.wav` audio file.
7.  The user receives the generated song.
